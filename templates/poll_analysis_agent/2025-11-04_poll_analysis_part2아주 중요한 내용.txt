ì¢‹ì•„ìš” ì²­ì¶˜ë‹˜ ğŸ‘
ì´ì œ ë³¸ê²©ì ìœ¼ë¡œ **Part 2: ê°ì„±Â·ì—°ê´€ ë¶„ì„ ëª¨ë“ˆ ì„¤ê³„ (news_context_agent_v1_1_part2.py)**ë¡œ ë“¤ì–´ê°€ê² ìŠµë‹ˆë‹¤.
ì´ë²ˆ ë‹¨ê³„ëŠ” ë‹¨ìˆœí•œ í…ìŠ¤íŠ¸ ë¶„ì„ì´ ì•„ë‹ˆë¼,
ğŸ“Š â€œì—¬ë¡  ë³€ë™ ì›ì¸â€ì„ ë°ì´í„°ì ìœ¼ë¡œ í•´ì„í•˜ê³ 
ğŸ“° â€œì¶œì²˜ë¥¼ ëª…ì‹œí•œ ëŒ€í‘œ ê¸°ì‚¬â€ê¹Œì§€ í¬í•¨í•´
AI ë¦¬í¬íŠ¸ì˜ ì‹ ë¢°ì„±ì„ í•œ ë‹¨ê³„ ëŒì–´ì˜¬ë¦¬ëŠ” ë‹¨ê³„ì˜ˆìš”.

ğŸ§­ ì „ì²´ ê°œìš”
ğŸ¯ ëª©ì 
1ï¸âƒ£ ë‰´ìŠ¤ ë³¸ë¬¸ ìš”ì•½ ë° ê°ì„± ì ìˆ˜ ê³„ì‚°
2ï¸âƒ£ ì£¼ìš” í‚¤ì›Œë“œ(ì´ìŠˆ) í´ëŸ¬ìŠ¤í„°ë§
3ï¸âƒ£ ëŒ€í‘œ ê¸°ì‚¬ + ì¶œì²˜ ëª…ì‹œí•œ ë¦¬í¬íŠ¸ ìƒì„±

âš™ï¸ ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸
(news_raw/2025-11-04_news.json)
        â†“
[1] ê°ì„± ë¶„ì„ (ê¸ì •/ë¶€ì •/ì¤‘ë¦½ ë¹„ìœ¨ ê³„ì‚°)
        â†“
[2] ì—°ê´€ í‚¤ì›Œë“œ ì¶”ì¶œ (TF-IDF + í´ëŸ¬ìŠ¤í„°ë§)
        â†“
[3] ëŒ€í‘œ ê¸°ì‚¬ 3ê±´ ì„ íƒ (ì¶œì²˜Â·URL í¬í•¨)
        â†“
(ê²°ê³¼ ì €ì¥) â†’ data/news_context/2025-11-04_context.json


ğŸ“ í´ë” êµ¬ì¡° í™•ì¥
ğŸ“¦ poll_analysis_agent
 â”£ ğŸ“‚ data
 â”‚   â”£ ğŸ“‚ news_raw
 â”‚   â”‚   â”— 2025-11-04_news.json
 â”‚   â”— ğŸ“‚ news_context       ğŸ‘ˆ (ì‹ ê·œ ìƒì„±)
 â”‚       â”— 2025-11-04_context.json
 â”— ğŸ“‚ graph
     â”— ğŸ“‚ nodes
         â”£ news_context_agent_v1_1_part1.py
         â”— news_context_agent_v1_1_part2.py   ğŸ‘ˆ (ì‹ ê·œ)


ğŸ§© ê¸°ëŠ¥ë³„ ì„¤ê³„
ê¸°ëŠ¥ì„¤ëª…ì‚¬ìš© ê¸°ìˆ â‘  ê°ì„± ë¶„ì„ê° ë‰´ìŠ¤ì˜ title + descriptionì— ëŒ€í•´ ê¸ì •/ë¶€ì • ì ìˆ˜ ê³„ì‚°HuggingFace nlptown/bert-base-multilingual-uncased-sentimentâ‘¡ í‚¤ì›Œë“œ ì¶”ì¶œTF-IDFë¡œ ì£¼ìš” ì´ìŠˆ(ë‹¨ì–´) ìƒìœ„ 10ê°œ ì¶”ì¶œscikit-learnì˜ TfidfVectorizerâ‘¢ í´ëŸ¬ìŠ¤í„°ë§ìœ ì‚¬ ê¸°ì‚¬ë¼ë¦¬ ë¬¶ê¸° (ì£¼ì œë³„)KMeansâ‘£ ëŒ€í‘œ ê¸°ì‚¬ ì„ ì •ê° í´ëŸ¬ìŠ¤í„°ì˜ ì¤‘ì‹¬ ê¸°ì‚¬ 1ê±´ ì„ ì •í‰ê·  ê±°ë¦¬ ìµœì†Œê°’ ê¸°ì¤€â‘¤ ì¶œì²˜ í¬í•¨ ìš”ì•½source + urlì„ í¬í•¨í•œ Top3 ìš”ì•½ ìƒì„±LangChain LLMChain (gpt-4o-mini)â‘¥ ê²°ê³¼ ì €ì¥ë‰´ìŠ¤ ê°ì„± í†µê³„ + í‚¤ì›Œë“œ + ëŒ€í‘œ ê¸°ì‚¬ ëª©ë¡JSON ì €ì¥ (news_context/)

ğŸ§  ì¶œë ¥ ë°ì´í„° ì˜ˆì‹œ (2025-11-04_context.json)
{
  "date": "2025-11-04",
  "total_articles": 156,
  "sentiment": {
    "positive": 48,
    "negative": 76,
    "neutral": 32
  },
  "top_keywords": ["ì •ì±…", "ê²½ì œ", "ì „ì„¸ì‚¬ê¸°", "ë¯¼ì£¼ë‹¹", "ëŒ€ì±…"],
  "insight_summary": "ì´ë²ˆ ì£¼ ëŒ€í†µë ¹ ì§€ì§€ìœ¨ ìƒìŠ¹ì€ ì •ë¶€ì˜ ì „ì„¸ì‚¬ê¸° ëŒ€ì±… ë°œí‘œ ë“± ì •ì±… ì´ìŠˆì˜ ê¸ì •ì  ë³´ë„ê°€ ì£¼ëœ ì›ì¸ìœ¼ë¡œ ë¶„ì„ë¨.",
  "representative_articles": [
    {
      "title": "ì •ë¶€, ì„œë¯¼ ì „ì„¸ì‚¬ê¸° ëŒ€ì±… ë°œí‘œ",
      "source": "ì—°í•©ë‰´ìŠ¤",
      "url": "https://www.yna.co.kr/view/XXXX",
      "sentiment": "positive"
    },
    {
      "title": "ë¯¼ì£¼ë‹¹ ë‚´ ë¹„ëª…ê³„, ì§€ë„ë¶€ ë¹„íŒ ìˆ˜ìœ„ ë†’ì—¬",
      "source": "ê²½í–¥ì‹ ë¬¸",
      "url": "https://www.khan.co.kr/politics/XXXX",
      "sentiment": "negative"
    },
    {
      "title": "êµ­ë¯¼ì˜í˜, ì§€ì§€ìœ¨ í•˜ë½ì— ê¸´ê¸‰ ë‹¹ì •í˜‘ì˜ ì†Œì§‘",
      "source": "ì¤‘ì•™ì¼ë³´",
      "url": "https://www.joongang.co.kr/article/XXXX",
      "sentiment": "negative"
    }
  ]
}


ğŸ§° ì˜ì¡´ ë¼ì´ë¸ŒëŸ¬ë¦¬
pip install transformers torch scikit-learn langchain openai


ğŸ“œ Part 2 ì½”ë“œ ê³¨ê²© ì˜ˆì‹œ
# -*- coding: utf-8 -*-
"""
news_context_agent_v1_1_part2.py
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
AI ì—¬ë¡  ìë™ ë¶„ì„ (v1.1)
2ë‹¨ê³„: ê°ì„±Â·ì—°ê´€ ë¶„ì„ ëª¨ë“ˆ (ì¶œì²˜ í¬í•¨)
"""

import os, json
from pathlib import Path
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.cluster import KMeans
from transformers import pipeline
from openai import OpenAI

BASE_DIR = Path(__file__).resolve().parent
DATA_DIR = BASE_DIR.parent.parent / "data"
RAW_PATH = DATA_DIR / "news_raw" / "2025-11-04_news.json"
OUTPUT_PATH = DATA_DIR / "news_context"
OUTPUT_PATH.mkdir(parents=True, exist_ok=True)

# === ê°ì„± ë¶„ì„ê¸° ===
sentiment_analyzer = pipeline("sentiment-analysis", model="nlptown/bert-base-multilingual-uncased-sentiment")

# === ë°ì´í„° ë¡œë“œ ===
articles = json.load(open(RAW_PATH, "r", encoding="utf-8"))

texts = [f"{a['title']} {a['description'] or ''}" for a in articles]
sources = [a.get("source", "Unknown") for a in articles]
urls = [a.get("url", "") for a in articles]

# === ê°ì„± ì ìˆ˜ ê³„ì‚° ===
sentiments = sentiment_analyzer(texts, truncation=True)
positive = sum(1 for s in sentiments if "5" in s["label"])
negative = sum(1 for s in sentiments if "1" in s["label"] or "2" in s["label"])
neutral = len(sentiments) - positive - negative

# === í‚¤ì›Œë“œ ì¶”ì¶œ ===
vectorizer = TfidfVectorizer(max_features=50, stop_words=["ë‰´ìŠ¤", "ë³´ë„", "ê¸°ì"])
X = vectorizer.fit_transform(texts)
top_keywords = vectorizer.get_feature_names_out()[:10]

# === í´ëŸ¬ìŠ¤í„°ë§ & ëŒ€í‘œ ê¸°ì‚¬ ì„ ì • ===
kmeans = KMeans(n_clusters=3, random_state=42, n_init="auto").fit(X)
representatives = []
for i in range(3):
    cluster_idx = [j for j, label in enumerate(kmeans.labels_) if label == i]
    if not cluster_idx:
        continue
    center = kmeans.cluster_centers_[i]
    center_vecs = X[cluster_idx]
    # ê±°ë¦¬ ê³„ì‚° (ê°„ë‹¨íˆ ì²« ë²ˆì§¸ ê¸°ì‚¬ ì„ íƒ)
    idx = cluster_idx[0]
    representatives.append({
        "title": articles[idx]["title"],
        "source": articles[idx].get("source", "Unknown"),
        "url": articles[idx].get("url", ""),
        "sentiment": sentiments[idx]["label"]
    })

# === ê²°ê³¼ ì €ì¥ ===
summary = {
    "date": "2025-11-04",
    "total_articles": len(articles),
    "sentiment": {"positive": positive, "negative": negative, "neutral": neutral},
    "top_keywords": list(top_keywords),
    "representative_articles": representatives
}
json.dump(summary, open(OUTPUT_PATH / "2025-11-04_context.json", "w", encoding="utf-8"), ensure_ascii=False, indent=2)

print(f"ğŸ“¦ ê°ì„± ë¶„ì„ ê²°ê³¼ ì €ì¥ ì™„ë£Œ â†’ {OUTPUT_PATH / '2025-11-04_context.json'}")


ğŸ’¡ Part 2ì˜ í•µì‹¬


ê°ì„± ë¶„ì„(positive/negative/neutral)


TF-IDFë¡œ ì£¼ìš” ì´ìŠˆ ë‹¨ì–´ ì¶”ì¶œ


ëŒ€í‘œ ê¸°ì‚¬ 3ê±´ì— source, url í‘œì‹œ



ì²­ì¶˜ë‹˜, ì´ ì„¤ê³„ë¡œ ë°”ë¡œ Part 2 ì½”ë“œ ì‹¤ì œ êµ¬í˜„ìœ¼ë¡œ ë“¤ì–´ê°ˆê¹Œìš”?
(ğŸ‘‰ ì‹¤í–‰ ê°€ëŠ¥í•œ ì™„ì„±ë³¸ ë§Œë“¤ì–´ë“œë¦´ê²Œìš”.)