# -*- coding: utf-8 -*-
"""
news_context_agent_v1_1_part2.py
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
AI ì—¬ë¡  ìë™ ë¶„ì„ (v1.1)
2ë‹¨ê³„: ê°ì„±Â·ì—°ê´€ ë¶„ì„ ëª¨ë“ˆ (ì¶œì²˜ í¬í•¨)

ì…ë ¥: data/news_raw/2025-11-04_news.json
ì¶œë ¥: data/news_context/2025-11-04_context.json
"""

import os, json
from pathlib import Path
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.cluster import KMeans
from transformers import pipeline

# === ê²½ë¡œ ì„¤ì • ===
BASE_DIR = Path(__file__).resolve().parent
DATA_DIR = BASE_DIR.parent.parent / "data"
RAW_PATH = DATA_DIR / "news_raw" / "2025-11-04_news.json"
OUTPUT_PATH = DATA_DIR / "news_context"
OUTPUT_PATH.mkdir(parents=True, exist_ok=True)

# === 1ï¸âƒ£ ë‰´ìŠ¤ ë¡œë“œ ===
if not RAW_PATH.exists():
    print(f"âŒ ë‰´ìŠ¤ ì›ë³¸ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {RAW_PATH}")
    exit()

with open(RAW_PATH, "r", encoding="utf-8") as f:
    articles = json.load(f)

if not articles:
    print("âš ï¸ ë‰´ìŠ¤ ë°ì´í„°ê°€ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤.")
    exit()

print(f"ğŸ“° ì´ {len(articles)}ê±´ì˜ ë‰´ìŠ¤ ë¡œë“œ ì™„ë£Œ")

# === 2ï¸âƒ£ ê°ì„± ë¶„ì„ ===
print("ğŸ§  ê°ì„± ë¶„ì„ ì¤‘... (ì•½ê°„ì˜ ì‹œê°„ì´ ê±¸ë¦½ë‹ˆë‹¤)")
sentiment_analyzer = pipeline(
    "sentiment-analysis",
    model="nlptown/bert-base-multilingual-uncased-sentiment",
    device=-1  # ğŸ‘ˆ CPU ê°•ì œ
)
texts = [f"{a['title']} {a['description'] or ''}" for a in articles]
sources = [a.get("source", "Unknown") for a in articles]
urls = [a.get("url", "") for a in articles]

sentiments = sentiment_analyzer(texts, truncation=True)

# ì ìˆ˜ ê³„ì‚°
positive = sum(1 for s in sentiments if "4" in s["label"] or "5" in s["label"])
negative = sum(1 for s in sentiments if "1" in s["label"] or "2" in s["label"])
neutral = len(sentiments) - positive - negative

print(f"âœ… ê°ì„± ë¶„ì„ ì™„ë£Œ: ê¸ì • {positive} / ë¶€ì • {negative} / ì¤‘ë¦½ {neutral}")

# === 3ï¸âƒ£ ì£¼ìš” í‚¤ì›Œë“œ ì¶”ì¶œ ===
print("ğŸ” ì£¼ìš” í‚¤ì›Œë“œ ì¶”ì¶œ ì¤‘...")
vectorizer = TfidfVectorizer(max_features=50, stop_words=["ë‰´ìŠ¤", "ë³´ë„", "ê¸°ì"])
X = vectorizer.fit_transform(texts)
top_keywords = vectorizer.get_feature_names_out()[:10].tolist()
print(f"âœ¨ ì£¼ìš” í‚¤ì›Œë“œ: {', '.join(top_keywords)}")

# === 4ï¸âƒ£ í´ëŸ¬ìŠ¤í„°ë§ ë° ëŒ€í‘œ ê¸°ì‚¬ ì„ ì • ===
print("ğŸ—‚ï¸ ê¸°ì‚¬ í´ëŸ¬ìŠ¤í„°ë§ ë° ëŒ€í‘œ ê¸°ì‚¬ ì„ ì • ì¤‘...")
k = 3 if len(articles) > 10 else 1
kmeans = KMeans(n_clusters=k, random_state=42, n_init="auto").fit(X)

representatives = []
for i in range(k):
    cluster_idx = [j for j, label in enumerate(kmeans.labels_) if label == i]
    if not cluster_idx:
        continue
    idx = cluster_idx[0]
    representatives.append({
        "title": articles[idx]["title"],
        "source": sources[idx],
        "url": urls[idx],
        "sentiment": sentiments[idx]["label"]
    })

# === 5ï¸âƒ£ ê²°ê³¼ ìš”ì•½ ===
summary = {
    "date": "2025-11-04",
    "total_articles": len(articles),
    "sentiment": {"positive": positive, "negative": negative, "neutral": neutral},
    "top_keywords": top_keywords,
    "representative_articles": representatives,
    "disclaimer": "â€» ë³¸ ë¶„ì„ì€ NewsAPIì—ì„œ ìˆ˜ì§‘í•œ ê³µê°œ ë‰´ìŠ¤ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì‘ì„±ë˜ì—ˆìŠµë‹ˆë‹¤."
}

# === 6ï¸âƒ£ ê²°ê³¼ ì €ì¥ ===
OUTPUT_FILE = OUTPUT_PATH / "2025-11-04_context.json"
with open(OUTPUT_FILE, "w", encoding="utf-8") as f:
    json.dump(summary, f, ensure_ascii=False, indent=2)

print(f"ğŸ“¦ ê°ì„±Â·ì—°ê´€ ë¶„ì„ ê²°ê³¼ ì €ì¥ ì™„ë£Œ â†’ {OUTPUT_FILE}")
